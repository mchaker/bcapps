# I am assuming the converted files are up-to-date; in theory could
# check them against the *-files.txt files but don't want to rerun
# portions of bc-unix-dump.pl unnecessarily

# converted file lists on all drives I recognize
converted=$(shell egrep -v '^\#|^$$' /home/barrycarter/BCGIT/BRIGHTON/mounts.txt | perl -anle 'print "$$F[0]/$$F[1]-converted.txt"')

# filelist.txt is the end product of all this
all: filelist.txt exclusions.txt exclusions-fgrep.txt

# afad = all files, all directories

afad.txt: ${converted}
	sort --parallel=1 ${converted} > afad.txt

# everything previously backed up, sorted
# TODO: assuming the *.exclude files are up-to-date, but should check for that

previouslydone.txt.srt:
	sort --parallel=1 -t '\0' -k2nr /root/massbacks-2020/*.exclude | sort --parallel=1 -t '\0' -k1,1 -u > previouslydone.txt.srt

# the files bc-chunk-backup2 uses to determine what to back up

backup0.txt: afad.txt previouslydone.txt.srt
	join --check-order -a 1 -t '\0' afad.txt previouslydone.txt.srt | sort --parallel=1 -t '\0' -k2nr > backup0.txt

# create exclusions egrep file from annotated file
exclusions.txt: exclusions-commented.txt
	egrep -v '^\#|^ *$$' exclusions-commented.txt | perl -pnle 's/\$$/\0/' > exclusions.txt

# create exclusions fgrep file from annotated file
exclusions-fgrep.txt: exclusions-fgrep-commented.txt
	egrep -v '^\#|^ *$$' exclusions-fgrep-commented.txt | perl -pnle 's/\$$/\0/' > exclusions-fgrep.txt

# limit 100G for mass 5TB backup (100T for testing)

filelist.txt: exclusions.txt backup0.txt exclusions-fgrep.txt
	fgrep -avf exclusions-fgrep.txt backup0.txt | egrep -avf exclusions.txt | bc-join-backup.pl | bc-chunk-backup2.pl --checkfile --limit=100,000,000,000 --debug
